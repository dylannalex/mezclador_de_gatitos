# Mezclador de Gatitos üê±

[![P√°gina Web](https://img.shields.io/badge/P%C3%A1gina_Web-Mezclador%20de%20Gatitos-blue)](https://mezclador-gatitos.streamlit.app/)
[![licence](https://img.shields.io/github/license/dylannalex/mezclador_de_gatitos?color=blue)](https://github.com/dylannalex/mezclador_de_gatitos/blob/main/LICENSE)

¬øAlguna vez has so√±ado con poder fusionar dos adorables gatitos en uno solo? ¬°Pues ahora puedes hacerlo realidad con el **Mezclador de Gatitos**! Este proyecto se basa en la inteligencia artificial y el poder de las redes neuronales para combinar im√°genes de gatitos, creando nuevas combinaciones visuales que no solo son asombrosas, sino tambi√©n irresistiblemente tiernas.

<p align="center"> <img src="../media/interpolation_example.png?raw=true" /> </p>

Con el Mezclador de Gatitos puedes:

- ‚ú® **Combinar im√°genes de gatitos:** Selecciona dos gatitos y descubre fascinantes fusiones llenas de ternura.

- ‚ú® **Descubrir el mapa oculto de los gatitos:** Explora c√≥mo la red neuronal organiza y representa a los gatitos en un espacio bidimensional.

**¬øListo para descubrir combinaciones que nunca imaginaste?** üëâ [Explora ahora y crea tus propios gatitos √∫nicos](https://mezclador-gatitos.streamlit.app/).

> El proyecto fue desarrollado como parte de la materia Redes Neuronales Profundas en la UTN FRM, bajo la gu√≠a del profesor Ing. Pablo Marinozi. Este trabajo es un testimonio de creatividad, aprendizaje y, por supuesto, ¬°mucha ternura! ‚ù§Ô∏è

## üìö Tabla de Contenidos

1. [Organizaci√≥n del Repositorio](#-organizaci√≥n-del-repositorio)
2. [Paquete para la implementaci√≥n del VAE](#-paquete-para-la-implementaci√≥n-del-vae)
3. [Instalaci√≥n y Ejecuci√≥n](#-instalaci√≥n-y-ejecuci√≥n)

## üêà Organizaci√≥n del Repositorio

El repositorio sigue una estructura organizada para facilitar el desarrollo, entrenamiento del modelo y ejecuci√≥n de la aplicaci√≥n:

### 1. `data/`
Contiene los datasets utilizados para el entrenamiento y evaluaci√≥n del modelo:

- **cats.zip**: Dataset con im√°genes de gatitos.
- **data_exploration.ipynb**: Notebook de Jupyter con an√°lisis y exploraci√≥n inicial de los datos.

### 2. `dev/`
Incluye notebooks y c√≥digo relacionado con el desarrollo experimental del modelo VAE:

- **src/**: C√≥digo fuente del Autoencoder Variacional (VAE).
- **model_training.ipynb**: Notebook donde se entrena el modelo utilizando el dataset de gatitos.

### 3. `prod/`
Carpeta destinada a la implementaci√≥n de la aplicaci√≥n final:

- **app.py**: Archivo principal que ejecuta la aplicaci√≥n web interactiva con Streamlit.
- **models/vae.pt**: Archivo con los pesos del modelo VAE entrenado en formato PyTorch.
- **requirements.txt**: Lista de dependencias necesarias para ejecutar el proyecto.
- **utils.py**: Funciones auxiliares para el preprocesamiento y carga del modelo.


## üêà Paquete para la implementaci√≥n del VAE

Para facilitar el entrenamiento de los modelos, el paquete `dev/src/` contiene tres funcionalidades clave:

### 1. VAE (Variational Autoencoder)

El modelo VAE implementa un codificador (encoder) y un decodificador (decoder) para comprimir y reconstruir datos. Este modelo utiliza el truco de reparametrizaci√≥n para aprender una distribuci√≥n latente que captura la estructura subyacente de los datos.

#### Principales par√°metros del constructor
- **image_h** (int): Altura de la imagen de entrada.
- **image_w** (int): Ancho de la imagen de entrada.
- **latent_dims** (int): Dimensionalidad del espacio latente.
- **hidden_channels** (list[int]): Lista con los canales ocultos de cada capa convolucional.
- **kernel_size** (int): Tama√±o del kernel para las convoluciones.
- **stride** (int): Stride (desplazamiento) utilizado en las convoluciones.
- **padding** (int): Padding aplicado en las convoluciones.
- **activation** (`nn.Module`): Funci√≥n de activaci√≥n utilizada en las capas.

#### M√©todos principales
- **encode(X)**: Codifica una entrada `X` y devuelve la representaci√≥n latente, junto con la media y la varianza logar√≠tmica.
- **decode(z)**: Decodifica una representaci√≥n latente `z` para reconstruir la entrada.
- **forward(X)**: Encadena las fases de codificaci√≥n y decodificaci√≥n en un √∫nico paso.
- **init_weights()**: Inicializa los pesos del modelo utilizando el m√©todo de inicializaci√≥n de Kaiming.

### 2. BetaLoss

Esta clase implementa una p√©rdida personalizada basada en la combinaci√≥n de la p√©rdida de reconstrucci√≥n y la p√©rdida latente ponderada por un factor $\beta$.

#### Principales par√°metros del constructor
- **beta** (float): Factor de ponderaci√≥n para la p√©rdida latente.

#### M√©todo principal:
- **forward(x, x_hat, mean, log_var)**
  - **x**: Entrada original.
  - **x_hat**: Salida reconstruida por el VAE.
  - **mean**: Media de la distribuci√≥n latente.
  - **log_var**: Varianza logar√≠tmica de la distribuci√≥n latente.
  - **Retorna**: La suma ponderada de la p√©rdida de reconstrucci√≥n y la p√©rdida latente.

### 3. Funci√≥n de entrenamiento

La funci√≥n `train()` entrena el modelo por un n√∫mero especificado de √©pocas, guarda checkpoints peri√≥dicamente y calcula las m√©tricas de validaci√≥n.

#### Principales par√°metros
- **model** (`nn.Module`): Modelo a entrenar.
- **train_loader** (`DataLoader`): DataLoader para los datos de entrenamiento.
- **val_loader** (`DataLoader` o `None`): DataLoader para los datos de validaci√≥n (opcional).
- **epochs** (int): N√∫mero total de √©pocas para entrenar.
- **epochs_per_checkpoint** (int): Intervalo de √©pocas para guardar checkpoints.
- **device** (str): Dispositivo donde se ejecutar√° el entrenamiento (e.g., `cpu` o `cuda`).
- **model_dir** (str): Directorio donde se guardar√°n los pesos y el historial.
- **optimizer** (`torch.optim.Optimizer`): Optimizador utilizado para actualizar los pesos.
- **loss_function** (`nn.Module`): Funci√≥n de p√©rdida utilizada en el entrenamiento.

#### Flujo principal
1. **Preparaci√≥n del modelo**:
   - Verifica si existen pesos previamente entrenados en el directorio especificado.
   - Carga los pesos m√°s recientes si est√°n disponibles.
2. **Entrenamiento**:
   - Por cada √©poca, entrena el modelo utilizando el `train_loader` y calcula el promedio de la p√©rdida.
   - Si se especifica un `val_loader`, calcula la p√©rdida promedio en el conjunto de validaci√≥n.
3. **Checkpointing**:
   - Guarda los pesos del modelo y actualiza las m√©tricas en un archivo `.csv` despu√©s de cada √©poca.

Esta estructura modular permite entrenar un modelo VAE de manera eficiente y realizar un seguimiento detallado del proceso.


## üêà Instalaci√≥n y Ejecuci√≥n

Para ejecutar el proyecto en tu m√°quina local, sigue estos pasos:

### 1. Clonar el repositorio

```bash
$ git clone https://github.com/dylannalex/mezclador_de_gatitos.git
```

### 2. Instalar dependencias
```bash
$ pip install -r prod/requirements.txt
```

### 3. Ejecutar la aplicaci√≥n
```bash
$ streamlit run prod/app.py
```

